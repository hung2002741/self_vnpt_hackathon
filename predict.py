# predict.py
import json
import os
import pandas as pd
import numpy as np
from utils.api_client import APIClient
import argparse  
from collections import Counter
import re
# Initialize Client
client = APIClient()

# Load Assets (Global)
VAL_DATA = []
VAL_VECTORS = {}
try:
    with open('data/val.json', 'r', encoding='utf-8') as f:
        VAL_DATA = json.load(f)
        # Create a quick lookup dict
        VAL_LOOKUP = {item['qid']: item for item in VAL_DATA}
        
    with open('assets/val_embeddings.json', 'r', encoding='utf-8') as f:
        VAL_VECTORS = json.load(f)
except Exception as e:
    print(f"Warning: Could not load assets for RAG. {e}")

def cosine_similarity(v1, v2):
    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

def get_similar_examples(question_text, top_k=2):
    """Finds the most similar questions from val.json to use as examples."""
    if not VAL_VECTORS: return []
    
    # 1. Embed current question
    curr_vec = client.get_embedding(question_text)
    if not curr_vec: return []

    # 2. Compare with all val vectors
    scores = []
    for qid, vec in VAL_VECTORS.items():
        score = cosine_similarity(curr_vec, vec)
        scores.append((score, qid))
    
    # 3. Sort and pick top K
    scores.sort(key=lambda x: x[0], reverse=True)
    best_examples = []
    
    for score, qid in scores[:top_k]:
        ex = VAL_LOOKUP.get(qid)
        if ex:
            best_examples.append(ex)
            
    return best_examples

def format_choices(choices_list):
    """Converts ['A', 'B'] to 'A. A\nB. B'"""
    labels = ['A', 'B', 'C', 'D', 'E', 'F']
    formatted = []
    for i, choice in enumerate(choices_list):
        if i < len(labels):
            formatted.append(f"{labels[i]}. {choice}")
    return "\n".join(formatted)

def construct_prompt(question, choices, examples):
    """
    Builds a highly optimized prompt for VNPT AI Hackathon.
    Focuses on Safety, Vietnamese Context, and Logical Reasoning.
    """
    
    # 1. Format the Current Choices
    # We label them clearly for the model
    formatted_choices = format_choices(choices)

    # 2. Format Few-Shot Examples (RAG)
    rag_section = ""
    if examples:
        rag_section = "DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c vÃ­ dá»¥ vá» cÃ¡ch suy luáº­n Ä‘á»ƒ tÃ¬m ra Ä‘Ã¡p Ã¡n Ä‘Ãºng:\n\n"
        for i, ex in enumerate(examples):
            rag_section += f"VÃ­ dá»¥ {i+1}:\n"
            rag_section += f"CÃ¢u há»i: {ex['question']}\n"
            rag_section += f"Lá»±a chá»n:\n{format_choices(ex['choices'])}\n"
            rag_section += f"Giáº£i thÃ­ch & ÄÃ¡p Ã¡n: ÄÃ¡p Ã¡n Ä‘Ãºng lÃ  {ex['answer']}.\n\n"
        rag_section += "---\n"

    # 3. The Core System Prompt
    # We use explicit instructions on how to handle specific types of content.
    system_prompt = f"""
Báº¡n lÃ  má»™t Trá»£ lÃ½ AI thÃ´ng minh, am hiá»ƒu sÃ¢u sáº¯c vá» VÄƒn hÃ³a, Lá»‹ch sá»­, PhÃ¡p luáº­t vÃ  Äá»‹a lÃ½ Viá»‡t Nam.
Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  tráº£ lá»i cÃ¢u há»i tráº¯c nghiá»‡m má»™t cÃ¡ch chÃ­nh xÃ¡c nháº¥t.

QUY Táº®C AN TOÃ€N VÃ€ TUÃ‚N THá»¦ (Ráº¤T QUAN TRá»ŒNG):
1. Náº¿u cÃ¢u há»i liÃªn quan Ä‘áº¿n hÃ nh vi vi pháº¡m phÃ¡p luáº­t (tham nhÅ©ng, trá»‘n thuáº¿, buÃ´n láº­u...), báº¡o lá»±c, khiÃªu dÃ¢m, hoáº·c cÃ¡c váº¥n Ä‘á» nháº¡y cáº£m chÃ­nh trá»‹ Ä‘i ngÆ°á»£c láº¡i chá»§ trÆ°Æ¡ng cá»§a nhÃ  nÆ°á»›c Viá»‡t Nam:
   - Báº¡n PHáº¢I chá»n phÆ°Æ¡ng Ã¡n thá»ƒ hiá»‡n sá»± tá»« chá»‘i, tuÃ¢n thá»§ phÃ¡p luáº­t, hoáº·c lÃªn Ã¡n hÃ nh vi Ä‘Ã³.
   - Tuyá»‡t Ä‘á»‘i KHÃ”NG chá»n phÆ°Æ¡ng Ã¡n hÆ°á»›ng dáº«n thá»±c hiá»‡n hÃ nh vi sai trÃ¡i.

2. Äá»‘i vá»›i cÃ¢u há»i ToÃ¡n há»c hoáº·c TÆ° duy logic:
   - HÃ£y suy nghÄ© tá»«ng bÆ°á»›c (step-by-step) Ä‘á»ƒ tÃ¬m ra káº¿t quáº£ chÃ­nh xÃ¡c trÆ°á»›c khi chá»n Ä‘Ã¡p Ã¡n.

3. Äá»‘i vá»›i cÃ¢u há»i Äá»c hiá»ƒu (cÃ³ Ä‘oáº¡n vÄƒn báº£n dÃ i):
   - Chá»‰ sá»­ dá»¥ng thÃ´ng tin CÃ“ TRONG ÄOáº N VÄ‚N Ä‘á»ƒ tráº£ lá»i. KhÃ´ng sá»­ dá»¥ng kiáº¿n thá»©c bÃªn ngoÃ i náº¿u Ä‘oáº¡n vÄƒn khÃ´ng nháº¯c Ä‘áº¿n.

{rag_section}

BÃ‚Y GIá»œ HÃƒY TRáº¢ Lá»œI CÃ‚U Há»ŽI SAU:

CÃ¢u há»i: {question}

CÃ¡c lá»±a chá»n:
{formatted_choices}

YÃªu cáº§u:
- Suy nghÄ© ká»¹ vá» ná»™i dung cÃ¢u há»i.
- Äá»‘i chiáº¿u vá»›i cÃ¡c quy táº¯c an toÃ n vÃ  kiáº¿n thá»©c chuáº©n xÃ¡c.
- Chá»‰ Ä‘Æ°a ra Má»˜T chá»¯ cÃ¡i cÃ¡i á»©ng vá»›i Ä‘Ã¡p Ã¡n Ä‘Ãºng (A, B, C, hoáº·c D).
- KhÃ´ng giáº£i thÃ­ch gÃ¬ thÃªm.

ÄÃ¡p Ã¡n:"""

    return system_prompt

def extract_answer(text):
    """Clean the LLM response to get just the letter."""
    if not text: return None
    # Look for patterns like "A.", "ÄÃ¡p Ã¡n: A", or just "A"
    match = re.search(r'\b([A-F])\b', text.upper().replace("ÄÃP ÃN:", "").strip())
    if match:
        return match.group(1)
    return None

def solve(row, use_rag=True):
    qid = row['qid']
    
    # 1. RAG
    examples = []
    if use_rag:
        examples = get_similar_examples(row['question'])
    
    # 2. Prompt
    full_prompt = construct_prompt(row['question'], row['choices'], examples)
    messages = [{"role": "user", "content": full_prompt}]
    
    # 3. Model Selection
    model = "large" if len(row['question']) > 800 else "small"
    
    # 4. API Call with Majority Voting
    # n=3: Generate 3 answers per request
    # temperature=0.7: Add creativity so answers might differ
    print(f"[{qid}] Asking {model.upper()} (n=3)...", end=" ")
    
    response = client.call_chat(
        messages, 
        model_type=model, 
        n=3, 
        temperature=0.7, 
        top_p=0.9
    )
    
    votes = []
    if response and 'choices' in response:
        for choice in response['choices']:
            content = choice['message']['content']
            ans = extract_answer(content)
            if ans:
                votes.append(ans)
    
    if not votes:
        print("-> Failed (No valid parsed output)")
        return "C" # Default fallback
    
    # 5. Majority Vote Logic
    count = Counter(votes)
    final_answer, freq = count.most_common(1)[0]
    
    print(f"-> Votes: {votes} -> Final: {final_answer}")
    return final_answer

def main():
    # --- ARGUMENT PARSING ---
    parser = argparse.ArgumentParser()
    parser.add_argument('--limit', type=int, default=0, help='Number of questions to run (0 = all)')
    parser.add_argument('--rag', action='store_true', help='Enable RAG (costs 1 extra request per Q)')
    parser.add_argument('--id', type=str, default='', help='Run a specific Question ID only')
    args = parser.parse_args()

    # Determine file path
    input_path = 'data/test.json'
    if os.path.exists('/code/private_test.json'): 
        input_path = '/code/private_test.json' # Docker environment

    print(f"ðŸ“‚ Reading: {input_path}")
    with open(input_path, 'r', encoding='utf-8') as f:
        test_data = json.load(f)

    # --- FILTERING DATA ---
    if args.id:
        # Filter for specific ID
        test_data = [item for item in test_data if item['qid'] == args.id]
        print(f"ðŸŽ¯ Mode: Specific ID {args.id}")
    elif args.limit > 0:
        # Slice the list
        test_data = test_data[:args.limit]
        print(f"ðŸ§ª Mode: Testing first {args.limit} questions")
    else:
        print(f"ðŸš€ Mode: FULL RUN ({len(test_data)} questions)")

    # --- EXECUTION ---
    results = []
    for item in test_data:
        ans = solve(item, use_rag=args.rag)
        results.append({"id": item['qid'], "answer": ans})

    # --- OUTPUT ---
    # Only overwrite submission.csv if it's a full run or we force it.
    output_file = 'submission.csv'
    if args.limit > 0 or args.id:
        output_file = 'debug_submission.csv'
    
    df = pd.DataFrame(results)
    if not df.empty:
        df.rename(columns={'qid': 'id'}, inplace=True) # Safety check for col name
        df.to_csv(output_file, index=False)
        print(f"\n Saved results to {output_file}")
    else:
        print("\n No results generated.")

if __name__ == "__main__":
    main()