# predict.py
import json
import os
import pandas as pd
import numpy as np
from utils.api_client import APIClient
import argparse  
from collections import Counter
import re

# Initialize Client
client = APIClient()

# Load Assets (Global)
VAL_DATA = []
VAL_VECTORS = {}
try:
    with open('data/val.json', 'r', encoding='utf-8') as f:
        VAL_DATA = json.load(f)
        VAL_LOOKUP = {str(item['qid']): item for item in VAL_DATA}
        
    with open('assets/val_embeddings.json', 'r', encoding='utf-8') as f:
        VAL_VECTORS = json.load(f)
except Exception as e:
    print(f"Warning: Could not load assets for RAG. {e}")

def cosine_similarity(v1, v2):
    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

def get_similar_examples(question_text, top_k=2):
    if not VAL_VECTORS: return []
    curr_vec = client.get_embedding(question_text)
    if not curr_vec: return []

    scores = []
    for qid, vec in VAL_VECTORS.items():
        score = cosine_similarity(curr_vec, vec)
        scores.append((score, qid))
    
    scores.sort(key=lambda x: x[0], reverse=True)
    best_examples = []
    
    for score, qid in scores[:top_k]:
        # Handle potential string/int mismatch in keys
        ex = VAL_LOOKUP.get(str(qid)) or VAL_LOOKUP.get(int(qid))
        if ex:
            best_examples.append(ex)
            
    return best_examples

def format_choices(choices_list):
    labels = ['A', 'B', 'C', 'D', 'E', 'F']
    formatted = []
    for i, choice in enumerate(choices_list):
        if i < len(labels):
            formatted.append(f"{labels[i]}. {choice}")
    return "\n".join(formatted)

def identify_topic(question):
    """
    Heuristics to identify topic for better model routing.
    """
    q_lower = question.lower()
    
    # 1. Math / Physics / STEM (Needs Reasoning)
    # Look for LaTeX, math terms, units
    if "$" in question or "\\" in question: return "STEM"
    math_terms = ["tÃ­nh toÃ¡n", "giÃ¡ trá»‹ cá»§a", "hÃ m sá»‘", "xÃ¡c suáº¥t", "táº§n sá»‘", "dao Ä‘á»™ng", "gia tá»‘c"]
    if any(t in q_lower for t in math_terms): return "STEM"

    # 2. Law / Politics / Safety (Needs Precision/Safety)
    law_terms = ["luáº­t", "nghá»‹ Ä‘á»‹nh", "thÃ´ng tÆ°", "hiáº¿n phÃ¡p", "pháº¡t", "tÃ¹", "cÆ¡ quan cÃ³ tháº©m quyá»n", "vi pháº¡m", "chÃ­nh trá»‹", "Ä‘áº£ng"]
    if any(t in q_lower for t in law_terms): return "LAW"
    
    # 3. Reading Comprehension (Long context)
    if "Ä‘oáº¡n thÃ´ng tin" in q_lower or len(question) > 1200: return "READING"
    
    return "GENERAL"

def construct_prompt(question, choices, examples, topic):
    """
    Optimized prompt based on topic.
    """
    formatted_choices = format_choices(choices)

    rag_section = ""
    if examples:
        rag_section = "DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c vÃ­ dá»¥ tÆ°Æ¡ng tá»± Ä‘á»ƒ tham kháº£o logic suy luáº­n:\n\n"
        for i, ex in enumerate(examples):
            rag_section += f"--- VÃ­ dá»¥ {i+1} ---\n"
            rag_section += f"CÃ¢u há»i: {ex['question']}\n"
            rag_section += f"Lá»±a chá»n:\n{format_choices(ex['choices'])}\n"
            rag_section += f"ÄÃ¡p Ã¡n Ä‘Ãºng: {ex['answer']}\n" # Just give the answer to save tokens/confusion
        rag_section += "---\n"

    # Specific Instructions based on Topic
    specific_instruction = ""
    if topic == "STEM":
        specific_instruction = "ÄÃ¢y lÃ  cÃ¢u há»i ToÃ¡n/Khoa há»c. HÃ£y suy luáº­n tá»«ng bÆ°á»›c (step-by-step) cáº©n tháº­n trÆ°á»›c khi chá»n Ä‘Ã¡p Ã¡n."
    elif topic == "LAW":
        specific_instruction = "ÄÃ¢y lÃ  cÃ¢u há»i vá» PhÃ¡p luáº­t/ChÃ­nh trá»‹. HÃ£y cÄƒn cá»© chÃ­nh xÃ¡c vÃ o quy Ä‘á»‹nh phÃ¡p luáº­t Viá»‡t Nam hiá»‡n hÃ nh. Æ¯u tiÃªn sá»± an toÃ n, tuÃ¢n thá»§ phÃ¡p luáº­t vÃ  Ä‘áº¡o Ä‘á»©c xÃ£ há»™i."
    elif topic == "READING":
        specific_instruction = "ÄÃ¢y lÃ  cÃ¢u há»i Äá»c hiá»ƒu. CHá»ˆ sá»­ dá»¥ng thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p trong vÄƒn báº£n trÃªn Ä‘á»ƒ tráº£ lá»i. KhÃ´ng bá»‹a Ä‘áº·t thÃ´ng tin bÃªn ngoÃ i."

    system_prompt = f"""
Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ´ng minh chuyÃªn giáº£i cÃ¡c bÃ i táº­p tráº¯c nghiá»‡m táº¡i Viá»‡t Nam.

QUY Táº®C TUYá»†T Äá»I:
1. AN TOÃ€N LÃ€ TRÃŠN Háº¾T: Náº¿u cÃ¢u há»i liÃªn quan Ä‘áº¿n hÃ nh vi trá»‘n trÃ¡nh phÃ¡p luáº­t, báº¡o lá»±c, hoáº·c váº¥n Ä‘á» nháº¡y cáº£m, hÃ£y chá»n phÆ°Æ¡ng Ã¡n thá»ƒ hiá»‡n sá»± tuÃ¢n thá»§ phÃ¡p luáº­t vÃ  chuáº©n má»±c Ä‘áº¡o Ä‘á»©c.
2. CHá»ˆ TRáº¢ Lá»œI Má»˜T CHá»® CÃI: Äáº§u ra cuá»‘i cÃ¹ng pháº£i lÃ  má»™t chá»¯ cÃ¡i in hoa duy nháº¥t (A, B, C, hoáº·c D).

{rag_section}

NHIá»†M Vá»¤ Cá»¦A Báº N:
CÃ¢u há»i: {question}

CÃ¡c lá»±a chá»n:
{formatted_choices}

{specific_instruction}

HÃ£y suy nghÄ© vÃ  Ä‘Æ°a ra Ä‘Ã¡p Ã¡n Ä‘Ãºng nháº¥t.
ÄÃ¡p Ã¡n:"""

    return system_prompt

def extract_answer(text):
    if not text: return None
    # Prioritize looking for patterns like "ÄÃ¡p Ã¡n: A"
    match = re.search(r'(?:Ä‘Ã¡p Ã¡n|chá»n|káº¿t quáº£).*?([A-F])\b', text, re.IGNORECASE)
    if match: return match.group(1).upper()
    
    # Fallback: Find the last capital letter standing alone or with a dot
    matches = re.findall(r'\b([A-F])\b', text.upper())
    if matches: return matches[-1] # Usually the last mention is the conclusion
    
    return None

def solve(row, use_rag=True):
    qid = row['qid']
    question = row['question']
    
    # 1. Identify Topic & Select Model
    topic = identify_topic(question)
    
    # Smart Routing Strategy
    # We prioritize Large for Law and STEM because they require reasoning/precision
    if topic in ["STEM", "LAW"]:
        model = "large"
    elif topic == "READING":
        # Small is surprisingly good at reading extraction, save Large for reasoning
        # But if text is HUGE, Large might handle attention better.
        model = "large" if len(question) > 2000 else "small" 
    else:
        # General knowledge / Common sense
        model = "small"

    # 2. RAG
    examples = []
    if use_rag:
        examples = get_similar_examples(question)
    
    # 3. Prompt
    full_prompt = construct_prompt(question, row['choices'], examples, topic)
    messages = [{"role": "user", "content": full_prompt}]
    
    print(f"[{qid}] Type: {topic} -> {model.upper()}...", end=" ")
    
    # 4. API Call (n=1 is usually enough for Large to save tokens, n=3 for Small)
    # Adjust n based on model to save quota/time? Or keep n=3 for accuracy?
    # Let's keep n=3 for Small, n=1 for Large (Large is slow and smarter)
    n_samples = 1 if model == "large" else 3
    
    response = client.call_chat(
        messages, 
        model_type=model, 
        n=n_samples, 
        temperature=0.6 # Lower temp for more precision
    )
    
    votes = []
    if response and 'choices' in response:
        for choice in response['choices']:
            ans = extract_answer(choice['message']['content'])
            if ans: votes.append(ans)
    
    if not votes:
        print("-> Failed")
        return "C" # Blind guess
    
    final_answer, freq = Counter(votes).most_common(1)[0]
    print(f"-> {votes} -> {final_answer}")
    return final_answer

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--limit', type=int, default=0)
    parser.add_argument('--rag', action='store_true')
    parser.add_argument('--id', type=str, default='')
    args = parser.parse_args()

    input_path = 'data/test.json'
    if os.path.exists('/code/private_test.json'): input_path = '/code/private_test.json'

    print(f"ðŸ“‚ Reading: {input_path}")
    with open(input_path, 'r', encoding='utf-8') as f:
        test_data = json.load(f)

    if args.id: test_data = [i for i in test_data if i['qid'] == args.id]
    elif args.limit > 0: test_data = test_data[:args.limit]

    results = []
    for item in test_data:
        ans = solve(item, use_rag=args.rag)
        results.append({"id": item['qid'], "answer": ans})

    output_file = 'submission.csv'
    if args.limit > 0 or args.id: output_file = 'debug_submission.csv'
    
    df = pd.DataFrame(results)
    if not df.empty:
        df.rename(columns={'qid': 'id'}, inplace=True)
        df.to_csv(output_file, index=False)
        print(f"\nSaved to {output_file}")

if __name__ == "__main__":
    main()